# pip install pymongo pandas
import copy
import json
import pandas as pd
from pymongo import MongoClient
from datetime import datetime

# ----------------------------
# CONFIG
# ----------------------------
MONGO_URI = "mongodb://localhost:27017"
DB_NAME = "your_db"
SRC_COLL = "source_collection"
TGT_COLL = "testdata_collection"

DATA_CSV = "/path/to/data.csv"
MAP_JSON = "/path/to/mapping.json"

# Your batch id to extract from source
SOURCE_BATCH_ID = "BATCH_20251216_001"

# Where is batchId in Mongo document?
# If batchId is top-level, keep "batchId".
# If it is nested, use full path like "meta.batchId"
BATCH_ID_PATH = "batchId"

SKIP_BLANKS = True
BATCH_SIZE = 500


# ----------------------------
# HELPERS
# ----------------------------
def set_by_path(doc, path, value):
    """Set value using exact dot-path (case preserved). Supports list indexes like a.0.b"""
    parts = path.split(".")
    cur = doc

    for i, part in enumerate(parts):
        last = i == len(parts) - 1

        if part.isdigit():
            idx = int(part)
            if not isinstance(cur, list):
                return
            while len(cur) <= idx:
                cur.append({})
            if last:
                cur[idx] = value
            else:
                if cur[idx] is None:
                    cur[idx] = {}
                cur = cur[idx]
        else:
            if not isinstance(cur, dict):
                return
            if last:
                cur[part] = value
            else:
                if part not in cur or cur[part] is None:
                    cur[part] = {}
                cur = cur[part]


def get_by_path(doc, path):
    """Get value using dot-path. Returns None if missing."""
    parts = path.split(".")
    cur = doc
    for part in parts:
        if isinstance(cur, dict):
            if part not in cur:
                return None
            cur = cur[part]
        elif isinstance(cur, list) and part.isdigit():
            idx = int(part)
            if idx >= len(cur):
                return None
            cur = cur[idx]
        else:
            return None
    return cur


def coerce(v):
    """Convert CSV string to python type (no case change)."""
    if v is None:
        return None
    if isinstance(v, str):
        s = v.strip()
        if s == "":
            return ""
        if s == "true":
            return True
        if s == "false":
            return False
        try:
            if "." in s:
                return float(s)
            return int(s)
        except Exception:
            return s
    return v


def mongo_stream_by_batch(coll, batch_path, batch_value):
    """
    Infinite generator:
    Reads only docs where batch_path == batch_value
    Restarts when cursor ends (so CSV can be longer than matching docs).
    """
    query = {batch_path: batch_value} if "." not in batch_path else None

    # If batch_path is nested (meta.batchId), Mongo query must use dot notation directly
    if query is None:
        query = {batch_path: batch_value}

    while True:
        cursor = coll.find(query, no_cursor_timeout=True).batch_size(200)
        try:
            for doc in cursor:
                yield doc
        finally:
            cursor.close()


# ----------------------------
# LOAD FILES
# ----------------------------
with open(MAP_JSON, "r") as f:
    mapping = json.load(f)   # CSV_HEADER -> exact mongo path

df = pd.read_csv(DATA_CSV, dtype=str).fillna("")
csv_cols = list(df.columns)

missing = [c for c in csv_cols if c not in mapping]
if missing:
    print("❌ Missing mapping for CSV columns in mapping.json:")
    for c in missing:
        print("  -", c)
    raise ValueError("Fix mapping.json before running")


# ----------------------------
# MAIN
# ----------------------------
client = MongoClient(MONGO_URI)
db = client[DB_NAME]
src = db[SRC_COLL]
tgt = db[TGT_COLL]

doc_gen = mongo_stream_by_batch(src, BATCH_ID_PATH, SOURCE_BATCH_ID)

batch = []
inserted = 0

TARGET_BATCH_ID = f"{SOURCE_BATCH_ID}-test"

for row_idx, row in df.iterrows():
    src_doc = next(doc_gen)

    new_doc = copy.deepcopy(src_doc)
    new_doc.pop("_id", None)  # avoid duplicate key in target

    # 1) Replace mapped fields from CSV
    for col in csv_cols:
        path = mapping[col]           # exact path from JSON mapping
        raw = row[col]

        if SKIP_BLANKS and isinstance(raw, str) and raw.strip() == "":
            continue

        set_by_path(new_doc, path, coerce(raw))

    # 2) Update batchId for inserted test data
    # Option-1 (recommended): force to SOURCE_BATCH_ID + "-test"
    set_by_path(new_doc, BATCH_ID_PATH, TARGET_BATCH_ID)

    # If you instead want to append "-test" to whatever is already in doc:
    # old_batch = get_by_path(new_doc, BATCH_ID_PATH)
    # if old_batch is not None:
    #     set_by_path(new_doc, BATCH_ID_PATH, f"{old_batch}-test")
    # else:
    #     set_by_path(new_doc, BATCH_ID_PATH, TARGET_BATCH_ID)

    # metadata
    new_doc["_test_meta"] = {
        "csv_row": int(row_idx),
        "source_batchId": SOURCE_BATCH_ID,
        "target_batchId": TARGET_BATCH_ID,
        "source_doc_id": str(src_doc.get("_id")),
        "created_at": datetime.utcnow()
    }

    batch.append(new_doc)

    if len(batch) >= BATCH_SIZE:
        tgt.insert_many(batch)
        inserted += len(batch)
        batch = []

if batch:
    tgt.insert_many(batch)
    inserted += len(batch)

print(f"✅ Done. Extract batchId={SOURCE_BATCH_ID} | Inserted batchId={TARGET_BATCH_ID} | Docs inserted={inserted}")
